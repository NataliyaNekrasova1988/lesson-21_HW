{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Denis-R-V/Homeworks/blob/main/Part_4/Task_4_RNN.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4\n",
    "- Сравнить LSTM, RNN и GRU на задаче предсказания части речи (качество предсказания, скорость обучения, время инференса модели)\n",
    "- *к первой задаче добавить bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузчик данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формирование кастомного класса DatasetSeq на базе класса Dataset\n",
    "class DatasetSeq(Dataset):\n",
    "    #def __init__(self, data_dir, train_lang='en'):                 # при хранении файла на диске в colab\n",
    "    def __init__(self, dataset_dir):\n",
    "\t#open file\n",
    "        #with open(data_dir + train_lang + '.train', 'r') as f:     # при хранении файла на диске в colab\n",
    "        with open(dataset_dir, 'r', encoding=\"utf-8\") as f:         # датасет читает исходный файл\n",
    "            train = f.read().split('\\n\\n')                          # и разбивает его на блока (абзацы)\n",
    "                                                                    # добавлена кодировка - Python ругается\n",
    "\n",
    "        # delete extra tag markup\n",
    "        train = [x for x in train if not '_ ' in x]\n",
    "\t    #init vocabs of tokens for encoding {<str> token: <int> id}\n",
    "        \n",
    "        # формирование словарей токенов: таргет, слова и буквы (можно обрабатывать слова или буквы)     \n",
    "        # паддинг выполняет роль как в свертках (привести последовательности к одной длине)\n",
    "        self.target_vocab = {'<pad>': 0} # {p: 1, a: 2, r: 3, pu: 4}\n",
    "        self.word_vocab = {'<pad>': 0} # {cat: 1, sat: 2, on: 3, mat: 4, '.': 5}\n",
    "        self.char_vocab = {'<pad>': 0} # {c: 1, a: 2, t: 3, ' ': 4, s: 5}\n",
    "\t    \n",
    "        # Cat sat on mat. -> [1, 2, 3, 4, 5]\n",
    "        # p    a  r  p pu -> [1, 2, 3, 1, 4]\n",
    "        # chars  -> [1, 2, 3, 4, 5, 2, 3, 4]\n",
    "\t    \n",
    "        #init encoded sequences lists (processed data)\n",
    "        self.encoded_sequences = []\n",
    "        self.encoded_targets = []\n",
    "        self.encoded_char_sequences = []\n",
    "        # n=1 because first value is padding\n",
    "        n_word = 1\n",
    "        n_target = 1\n",
    "        n_char = 1\n",
    "        # для каждого абзаца\n",
    "        for line in train:\n",
    "            # последовательность, таргет и символы - пустые списки\n",
    "            sequence = []\n",
    "            target = []\n",
    "            chars = []\n",
    "            # для каждой строки абзаца\n",
    "            for item in line.split('\\n'):\n",
    "                # если строка не пуста\n",
    "                if item != '':\n",
    "                    # по пробелу разбиваем строку на слово и таргет\n",
    "                    word, label = item.split(' ')\n",
    "                    # если слова нет в словаре, оно добавляется в словарь с очередным номером (ключем/индексом)\n",
    "                    if self.word_vocab.get(word) is None:\n",
    "                        self.word_vocab[word] = n_word\n",
    "                        n_word += 1\n",
    "                    # если таргета нет в словаре, он добавляется в словарь с очередным номером (ключем/индексом)\n",
    "                    if self.target_vocab.get(label) is None:\n",
    "                        self.target_vocab[label] = n_target\n",
    "                        n_target += 1\n",
    "                    # для каждого символа в слове\n",
    "                    for char in word:\n",
    "                        # если символа нет в словаре, он добавляется в словарь с очередным номером (ключем/индексом)\n",
    "                        if self.char_vocab.get(char) is None:\n",
    "                            self.char_vocab[char] = n_char\n",
    "                            n_char += 1\n",
    "                    # к последовательности, таргету и символам добавляются слово, таргет и символы\n",
    "                    sequence.append(self.word_vocab[word])\n",
    "                    target.append(self.target_vocab[label])\n",
    "                    chars.append([self.char_vocab[char] for char in word])\n",
    "            # к словарям токенов добавляются полученные ранее последовательность, таргет и символы\n",
    "            self.encoded_sequences.append(sequence)\n",
    "            self.encoded_targets.append(target)\n",
    "            self.encoded_char_sequences.append(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'data': self.encoded_sequences[index], # [1, 2, 3, 4, 6] len=5\n",
    "            'char': self.encoded_char_sequences[index],# [[1,2,3], [4,5], [1,2], [2,6,5,4], []] len=5\n",
    "            'target': self.encoded_targets[index], # [1, 2, 3, 4, 6] len=5\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [67, 91, 92, 60, 61, 62, 15, 93, 21, 94, 95, 96, 26],\n",
       " 'char': [[33, 22, 15],\n",
       "  [25, 22, 4, 3, 20],\n",
       "  [28, 2, 6],\n",
       "  [23, 15, 4, 13, 34],\n",
       "  [3, 5, 13],\n",
       "  [23, 30],\n",
       "  [25, 22, 15],\n",
       "  [22, 15, 2, 20],\n",
       "  [18, 17],\n",
       "  [2, 13],\n",
       "  [4, 13, 42, 15, 6, 25, 12, 15, 13, 25],\n",
       "  [17, 4, 3, 12],\n",
       "  [31]],\n",
       " 'target': [6, 3, 8, 8, 5, 7, 6, 4, 7, 6, 4, 4, 2]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = 'en2.train'\n",
    "dataset = DatasetSeq(dataset_dir)\n",
    "dataset.__getitem__(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формирование батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = []\n",
    "    target = []\n",
    "    for item in batch:\n",
    "        data.append(torch.as_tensor(item['data']))\n",
    "        target.append(torch.as_tensor(item['target']))\n",
    "    # pad_sequence встроенная функция по дополнению батчей паддингом\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    target = pad_sequence(target, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {'data': data, 'target': target}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dataset.word_vocab) + 1\n",
    "n_classes = len(dataset.target_vocab) + 1\n",
    "n_chars = len(dataset.char_vocab) + 1\n",
    "emb_dim = 256\n",
    "hidden = 256\n",
    "n_epochs = 10\n",
    "batch_size = 64               \n",
    "cuda_device = 0\n",
    "device = f'cuda:{cuda_device}' if cuda_device != -1 else 'mps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        # делается эмбеддинг последовательности и целиком передается в LSTM\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.clf = nn.Linear(hidden_dim, n_classes)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.word_emb(x) # B x T x Emb_dim\n",
    "        # последовательность целиком передается в LSTM\n",
    "        hidden, _ = self.rnn(emb)   # B x T x Hid, B x 1 x Hid\n",
    "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация модели, задание оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = LSTMPredictor(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
    "LSTM_model.train()\n",
    "optim = torch.optim.Adam(LSTM_model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трейн луп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.986635446548462\n",
      "epoch: 0, step: 100, loss: 0.40807098150253296\n",
      "epoch: 0, step: 200, loss: 0.23439031839370728\n",
      "epoch: 0, step: 300, loss: 0.15080615878105164\n",
      "epoch: 1, step: 0, loss: 0.15749505162239075\n",
      "epoch: 1, step: 100, loss: 0.13533201813697815\n",
      "epoch: 1, step: 200, loss: 0.13321822881698608\n",
      "epoch: 1, step: 300, loss: 0.09757743775844574\n",
      "epoch: 2, step: 0, loss: 0.10443088412284851\n",
      "epoch: 2, step: 100, loss: 0.07949259877204895\n",
      "epoch: 2, step: 200, loss: 0.10325685143470764\n",
      "epoch: 2, step: 300, loss: 0.10741005837917328\n",
      "epoch: 3, step: 0, loss: 0.0762549415230751\n",
      "epoch: 3, step: 100, loss: 0.10484195500612259\n",
      "epoch: 3, step: 200, loss: 0.05027348920702934\n",
      "epoch: 3, step: 300, loss: 0.05924995243549347\n",
      "epoch: 4, step: 0, loss: 0.05131750926375389\n",
      "epoch: 4, step: 100, loss: 0.051714491099119186\n",
      "epoch: 4, step: 200, loss: 0.05728011205792427\n",
      "epoch: 4, step: 300, loss: 0.05422991141676903\n",
      "epoch: 5, step: 0, loss: 0.05132881924510002\n",
      "epoch: 5, step: 100, loss: 0.05386722460389137\n",
      "epoch: 5, step: 200, loss: 0.0513140968978405\n",
      "epoch: 5, step: 300, loss: 0.05619179829955101\n",
      "epoch: 6, step: 0, loss: 0.042312026023864746\n",
      "epoch: 6, step: 100, loss: 0.03791012614965439\n",
      "epoch: 6, step: 200, loss: 0.0456751249730587\n",
      "epoch: 6, step: 300, loss: 0.05169256031513214\n",
      "epoch: 7, step: 0, loss: 0.04326801002025604\n",
      "epoch: 7, step: 100, loss: 0.0355142243206501\n",
      "epoch: 7, step: 200, loss: 0.027132362127304077\n",
      "epoch: 7, step: 300, loss: 0.029726414009928703\n",
      "epoch: 8, step: 0, loss: 0.027861852198839188\n",
      "epoch: 8, step: 100, loss: 0.03049902431666851\n",
      "epoch: 8, step: 200, loss: 0.0280656348913908\n",
      "epoch: 8, step: 300, loss: 0.03090224415063858\n",
      "epoch: 9, step: 0, loss: 0.01657625287771225\n",
      "epoch: 9, step: 100, loss: 0.018420221284031868\n",
      "epoch: 9, step: 200, loss: 0.01492292620241642\n",
      "epoch: 9, step: 300, loss: 0.0173798855394125\n",
      "Время обучения модели LSTM: 0:03:09.865320\n",
      "Итоговый loss LSTM на обучающей выборке: 0.031768959015607834\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size, \n",
    "                            shuffle=True, \n",
    "                            collate_fn=collate_fn,\n",
    "                            drop_last = True,\n",
    "                            )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = LSTM_model(batch['data'].to(device))\n",
    "        loss = loss_func(predict.view(-1, n_classes),           # loss function ожидает число предиктов и число классов\n",
    "                         batch['target'].to(device).view(-1),   # батч таргета вытягивается в 1 длинный тензор\n",
    "                         )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "LSTM_train_time = datetime.datetime.now() - start\n",
    "LSTM_train_loss = loss.item()\n",
    "print(f\"Время обучения модели LSTM: {LSTM_train_time}\")\n",
    "print(f\"Итоговый loss LSTM на обучающей выборке: {LSTM_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', 'PUNCT', 'PROPN', 'PUNCT', 'ADP', 'ADV', 'PRON', 'VERB', 'PRON']\n",
      "Время инференса модели LSTM: 0:00:00.004986\n"
     ]
    }
   ],
   "source": [
    "phrase = 'So , Marius . At last I find you'\n",
    "words = phrase.split(' ')\n",
    "tokens = [dataset.word_vocab[w] for w in words]\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    LSTM_model.eval()\n",
    "    predict = LSTM_model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
    "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    LSTM_inference_time = datetime.datetime.now() - start\n",
    "\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l] for l in labels])\n",
    "print(f'Время инференса модели LSTM: {LSTM_inference_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        # делается эмбеддинг последовательности и целиком передается в RNN\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.clf = nn.Linear(hidden_dim, n_classes)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.word_emb(x) # B x T x Emb_dim\n",
    "        # последовательность целиком передается в RNN\n",
    "        hidden, _ = self.rnn(emb)   # B x T x Hid, B x 1 x Hid\n",
    "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация модели, задание оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = RNNPredictor(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
    "RNN_model.train()\n",
    "optim = torch.optim.Adam(RNN_model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трейн луп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.875800609588623\n",
      "epoch: 0, step: 100, loss: 0.3565364181995392\n",
      "epoch: 0, step: 200, loss: 0.25201037526130676\n",
      "epoch: 0, step: 300, loss: 0.09936951100826263\n",
      "epoch: 1, step: 0, loss: 0.17948633432388306\n",
      "epoch: 1, step: 100, loss: 0.21085622906684875\n",
      "epoch: 1, step: 200, loss: 0.15002746880054474\n",
      "epoch: 1, step: 300, loss: 0.10990066081285477\n",
      "epoch: 2, step: 0, loss: 0.12464043498039246\n",
      "epoch: 2, step: 100, loss: 0.0723038911819458\n",
      "epoch: 2, step: 200, loss: 0.10167223960161209\n",
      "epoch: 2, step: 300, loss: 0.09535925835371017\n",
      "epoch: 3, step: 0, loss: 0.11156997829675674\n",
      "epoch: 3, step: 100, loss: 0.08491683006286621\n",
      "epoch: 3, step: 200, loss: 0.1310947835445404\n",
      "epoch: 3, step: 300, loss: 0.09874419867992401\n",
      "epoch: 4, step: 0, loss: 0.07996009290218353\n",
      "epoch: 4, step: 100, loss: 0.08710276335477829\n",
      "epoch: 4, step: 200, loss: 0.07596481591463089\n",
      "epoch: 4, step: 300, loss: 0.043718572705984116\n",
      "epoch: 5, step: 0, loss: 0.05649275705218315\n",
      "epoch: 5, step: 100, loss: 0.07378727197647095\n",
      "epoch: 5, step: 200, loss: 0.053365644067525864\n",
      "epoch: 5, step: 300, loss: 0.07346523553133011\n",
      "epoch: 6, step: 0, loss: 0.04898710921406746\n",
      "epoch: 6, step: 100, loss: 0.03326180949807167\n",
      "epoch: 6, step: 200, loss: 0.04888547956943512\n",
      "epoch: 6, step: 300, loss: 0.0583878830075264\n",
      "epoch: 7, step: 0, loss: 0.05724714696407318\n",
      "epoch: 7, step: 100, loss: 0.08317209780216217\n",
      "epoch: 7, step: 200, loss: 0.048027656972408295\n",
      "epoch: 7, step: 300, loss: 0.0511089488863945\n",
      "epoch: 8, step: 0, loss: 0.04015876725316048\n",
      "epoch: 8, step: 100, loss: 0.03480100259184837\n",
      "epoch: 8, step: 200, loss: 0.0491454117000103\n",
      "epoch: 8, step: 300, loss: 0.046514157205820084\n",
      "epoch: 9, step: 0, loss: 0.02831030823290348\n",
      "epoch: 9, step: 100, loss: 0.033423133194446564\n",
      "epoch: 9, step: 200, loss: 0.04204350337386131\n",
      "epoch: 9, step: 300, loss: 0.030184663832187653\n",
      "Время обучения модели RNN: 0:02:41.726528\n",
      "Итоговый loss RNN на обучающей выборке: 0.04123825952410698\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size, \n",
    "                            shuffle=True, \n",
    "                            collate_fn=collate_fn,\n",
    "                            drop_last = True,\n",
    "                            )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = RNN_model(batch['data'].to(device))\n",
    "        loss = loss_func(predict.view(-1, n_classes),           # loss function ожидает число предиктов и число классов\n",
    "                         batch['target'].to(device).view(-1),   # батч таргета вытягивается в 1 длинный тензор\n",
    "                         )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "RNN_train_time = datetime.datetime.now() - start\n",
    "RNN_train_loss = loss.item()\n",
    "print(f\"Время обучения модели RNN: {RNN_train_time}\")\n",
    "print(f\"Итоговый loss RNN на обучающей выборке: {RNN_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', 'PUNCT', 'PROPN', 'PUNCT', 'ADP', 'ADV', 'PRON', 'VERB', 'PRON']\n",
      "Время инференса модели RNN: 0:00:00.003990\n"
     ]
    }
   ],
   "source": [
    "phrase = 'So , Marius . At last I find you'\n",
    "words = phrase.split(' ')\n",
    "tokens = [dataset.word_vocab[w] for w in words]\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    RNN_model.eval()\n",
    "    predict = RNN_model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
    "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    RNN_inference_time = datetime.datetime.now() - start\n",
    "\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l] for l in labels])\n",
    "print(f'Время инференса модели RNN: {RNN_inference_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        # делается эмбеддинг последовательности и целиком передается в RNN\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.clf = nn.Linear(hidden_dim, n_classes)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.word_emb(x) # B x T x Emb_dim\n",
    "        # последовательность целиком передается в RNN\n",
    "        hidden, _ = self.rnn(emb)   # B x T x Hid, B x 1 x Hid\n",
    "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация модели, задание оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_model = GRUPredictor(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
    "GRU_model.train()\n",
    "optim = torch.optim.Adam(GRU_model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трейн луп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.925163745880127\n",
      "epoch: 0, step: 100, loss: 0.39371463656425476\n",
      "epoch: 0, step: 200, loss: 0.2202056348323822\n",
      "epoch: 0, step: 300, loss: 0.22839149832725525\n",
      "epoch: 1, step: 0, loss: 0.19430921971797943\n",
      "epoch: 1, step: 100, loss: 0.16816985607147217\n",
      "epoch: 1, step: 200, loss: 0.15221503376960754\n",
      "epoch: 1, step: 300, loss: 0.12427293509244919\n",
      "epoch: 2, step: 0, loss: 0.041089266538619995\n",
      "epoch: 2, step: 100, loss: 0.12064767628908157\n",
      "epoch: 2, step: 200, loss: 0.09291797131299973\n",
      "epoch: 2, step: 300, loss: 0.07890412956476212\n",
      "epoch: 3, step: 0, loss: 0.0991993248462677\n",
      "epoch: 3, step: 100, loss: 0.055127453058958054\n",
      "epoch: 3, step: 200, loss: 0.08650507032871246\n",
      "epoch: 3, step: 300, loss: 0.09460203349590302\n",
      "epoch: 4, step: 0, loss: 0.053088631480932236\n",
      "epoch: 4, step: 100, loss: 0.07211247086524963\n",
      "epoch: 4, step: 200, loss: 0.0578676201403141\n",
      "epoch: 4, step: 300, loss: 0.058903325349092484\n",
      "epoch: 5, step: 0, loss: 0.06764907389879227\n",
      "epoch: 5, step: 100, loss: 0.04245107248425484\n",
      "epoch: 5, step: 200, loss: 0.04106908291578293\n",
      "epoch: 5, step: 300, loss: 0.07257095724344254\n",
      "epoch: 6, step: 0, loss: 0.0374750941991806\n",
      "epoch: 6, step: 100, loss: 0.05401339381933212\n",
      "epoch: 6, step: 200, loss: 0.031515032052993774\n",
      "epoch: 6, step: 300, loss: 0.03940676152706146\n",
      "epoch: 7, step: 0, loss: 0.046211179345846176\n",
      "epoch: 7, step: 100, loss: 0.02474197931587696\n",
      "epoch: 7, step: 200, loss: 0.036467596888542175\n",
      "epoch: 7, step: 300, loss: 0.025717467069625854\n",
      "epoch: 8, step: 0, loss: 0.029941601678729057\n",
      "epoch: 8, step: 100, loss: 0.03197938948869705\n",
      "epoch: 8, step: 200, loss: 0.0429757721722126\n",
      "epoch: 8, step: 300, loss: 0.0285629965364933\n",
      "epoch: 9, step: 0, loss: 0.02739104814827442\n",
      "epoch: 9, step: 100, loss: 0.03223560005426407\n",
      "epoch: 9, step: 200, loss: 0.02074296586215496\n",
      "epoch: 9, step: 300, loss: 0.02410663478076458\n",
      "Время обучения модели GRU: 0:03:10.953375\n",
      "Итоговый loss GRU на обучающей выборке: 0.02720639854669571\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size, \n",
    "                            shuffle=True, \n",
    "                            collate_fn=collate_fn,\n",
    "                            drop_last = True,\n",
    "                            )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = GRU_model(batch['data'].to(device))\n",
    "        loss = loss_func(predict.view(-1, n_classes),           # loss function ожидает число предиктов и число классов\n",
    "                         batch['target'].to(device).view(-1),   # батч таргета вытягивается в 1 длинный тензор\n",
    "                         )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "GRU_train_time = datetime.datetime.now() - start\n",
    "GRU_train_loss = loss.item()\n",
    "print(f\"Время обучения модели GRU: {GRU_train_time}\")\n",
    "print(f\"Итоговый loss GRU на обучающей выборке: {GRU_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', 'PUNCT', 'PROPN', 'PUNCT', 'ADP', 'ADV', 'PRON', 'VERB', 'PRON']\n",
      "Время инференса модели GRU: 0:00:00.001995\n"
     ]
    }
   ],
   "source": [
    "phrase = 'So , Marius . At last I find you'\n",
    "words = phrase.split(' ')\n",
    "tokens = [dataset.word_vocab[w] for w in words]\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    GRU_model.eval()\n",
    "    predict = GRU_model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
    "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    GRU_inference_time = datetime.datetime.now() - start\n",
    "\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l] for l in labels])\n",
    "print(f'Время инференса модели GRU: {GRU_inference_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirLSTMPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        # делается эмбеддинг последовательности и целиком передается в RNN\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.clf = nn.Linear(hidden_dim * 2, n_classes)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.word_emb(x) # B x T x Emb_dim\n",
    "        # последовательность целиком передается в RNN\n",
    "        hidden, _ = self.rnn(emb)   # B x T x Hid, B x 1 x Hid\n",
    "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация модели, задание оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BidirLSTM_model = BidirLSTMPredictor(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
    "BidirLSTM_model.train()\n",
    "optim = torch.optim.Adam(BidirLSTM_model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трейн луп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.974355936050415\n",
      "epoch: 0, step: 100, loss: 0.28779786825180054\n",
      "epoch: 0, step: 200, loss: 0.23163756728172302\n",
      "epoch: 0, step: 300, loss: 0.15558332204818726\n",
      "epoch: 1, step: 0, loss: 0.09718077629804611\n",
      "epoch: 1, step: 100, loss: 0.14829786121845245\n",
      "epoch: 1, step: 200, loss: 0.11599984019994736\n",
      "epoch: 1, step: 300, loss: 0.07546371221542358\n",
      "epoch: 2, step: 0, loss: 0.07108955830335617\n",
      "epoch: 2, step: 100, loss: 0.07560165971517563\n",
      "epoch: 2, step: 200, loss: 0.07052402198314667\n",
      "epoch: 2, step: 300, loss: 0.0738890990614891\n",
      "epoch: 3, step: 0, loss: 0.048259954899549484\n",
      "epoch: 3, step: 100, loss: 0.04757946729660034\n",
      "epoch: 3, step: 200, loss: 0.053839653730392456\n",
      "epoch: 3, step: 300, loss: 0.06078903004527092\n",
      "epoch: 4, step: 0, loss: 0.030623503029346466\n",
      "epoch: 4, step: 100, loss: 0.0337982140481472\n",
      "epoch: 4, step: 200, loss: 0.03134100139141083\n",
      "epoch: 4, step: 300, loss: 0.02424294501543045\n",
      "epoch: 5, step: 0, loss: 0.025052865967154503\n",
      "epoch: 5, step: 100, loss: 0.025320684537291527\n",
      "epoch: 5, step: 200, loss: 0.023659557104110718\n",
      "epoch: 5, step: 300, loss: 0.02175428718328476\n",
      "epoch: 6, step: 0, loss: 0.010542260482907295\n",
      "epoch: 6, step: 100, loss: 0.014791084453463554\n",
      "epoch: 6, step: 200, loss: 0.007257021497935057\n",
      "epoch: 6, step: 300, loss: 0.015714557841420174\n",
      "epoch: 7, step: 0, loss: 0.00783225242048502\n",
      "epoch: 7, step: 100, loss: 0.016133619472384453\n",
      "epoch: 7, step: 200, loss: 0.012290646322071552\n",
      "epoch: 7, step: 300, loss: 0.005628851242363453\n",
      "epoch: 8, step: 0, loss: 0.00403311662375927\n",
      "epoch: 8, step: 100, loss: 0.006846301723271608\n",
      "epoch: 8, step: 200, loss: 0.0066278595477342606\n",
      "epoch: 8, step: 300, loss: 0.010327912867069244\n",
      "epoch: 9, step: 0, loss: 0.004460535477846861\n",
      "epoch: 9, step: 100, loss: 0.0033546355552971363\n",
      "epoch: 9, step: 200, loss: 0.0038199815899133682\n",
      "epoch: 9, step: 300, loss: 0.004503894131630659\n",
      "Время обучения модели GRU: 0:05:12.256995\n",
      "Итоговый loss GRU на обучающей выборке: 0.005682139191776514\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size, \n",
    "                            shuffle=True, \n",
    "                            collate_fn=collate_fn,\n",
    "                            drop_last = True,\n",
    "                            )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = BidirLSTM_model(batch['data'].to(device))\n",
    "        loss = loss_func(predict.view(-1, n_classes),           # loss function ожидает число предиктов и число классов\n",
    "                         batch['target'].to(device).view(-1),   # батч таргета вытягивается в 1 длинный тензор\n",
    "                         )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "BidirLSTM_train_time = datetime.datetime.now() - start\n",
    "BidirLSTM_train_loss = loss.item()\n",
    "print(f\"Время обучения модели LSTM: {BidirLSTM_train_time}\")\n",
    "print(f\"Итоговый loss LSTM на обучающей выборке: {BidirLSTM_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', 'PUNCT', 'PROPN', 'PUNCT', 'ADP', 'ADV', 'PRON', 'VERB', 'PRON']\n",
      "Время инференса модели RNN: 0:00:00.005983\n"
     ]
    }
   ],
   "source": [
    "phrase = 'So , Marius . At last I find you'\n",
    "words = phrase.split(' ')\n",
    "tokens = [dataset.word_vocab[w] for w in words]\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    BidirLSTM_model.eval()\n",
    "    predict = BidirLSTM_model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
    "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    BidirLSTM_inference_time = datetime.datetime.now() - start\n",
    "\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l] for l in labels])\n",
    "print(f'Время инференса модели LSTM: {BidirLSTM_inference_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirRNNPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        # делается эмбеддинг последовательности и целиком передается в RNN\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.clf = nn.Linear(hidden_dim * 2, n_classes)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.word_emb(x) # B x T x Emb_dim\n",
    "        # последовательность целиком передается в RNN\n",
    "        hidden, _ = self.rnn(emb)   # B x T x Hid, B x 1 x Hid\n",
    "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация модели, задание оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BidirRNN_model = BidirRNNPredictor(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
    "BidirRNN_model.train()\n",
    "optim = torch.optim.Adam(BidirRNN_model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трейн луп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 3.1870741844177246\n",
      "epoch: 0, step: 100, loss: 0.25928953289985657\n",
      "epoch: 0, step: 200, loss: 0.18400955200195312\n",
      "epoch: 0, step: 300, loss: 0.1998547911643982\n",
      "epoch: 1, step: 0, loss: 0.1704816222190857\n",
      "epoch: 1, step: 100, loss: 0.10727096349000931\n",
      "epoch: 1, step: 200, loss: 0.1522814929485321\n",
      "epoch: 1, step: 300, loss: 0.10622724145650864\n",
      "epoch: 2, step: 0, loss: 0.11479531228542328\n",
      "epoch: 2, step: 100, loss: 0.08860860019922256\n",
      "epoch: 2, step: 200, loss: 0.09459187835454941\n",
      "epoch: 2, step: 300, loss: 0.07238802313804626\n",
      "epoch: 3, step: 0, loss: 0.05111191049218178\n",
      "epoch: 3, step: 100, loss: 0.058269061148166656\n",
      "epoch: 3, step: 200, loss: 0.0430125892162323\n",
      "epoch: 3, step: 300, loss: 0.08022809028625488\n",
      "epoch: 4, step: 0, loss: 0.05248823016881943\n",
      "epoch: 4, step: 100, loss: 0.030445965006947517\n",
      "epoch: 4, step: 200, loss: 0.03769374638795853\n",
      "epoch: 4, step: 300, loss: 0.06707563251256943\n",
      "epoch: 5, step: 0, loss: 0.03551698848605156\n",
      "epoch: 5, step: 100, loss: 0.050276946276426315\n",
      "epoch: 5, step: 200, loss: 0.04066086560487747\n",
      "epoch: 5, step: 300, loss: 0.04210738465189934\n",
      "epoch: 6, step: 0, loss: 0.025038540363311768\n",
      "epoch: 6, step: 100, loss: 0.027471568435430527\n",
      "epoch: 6, step: 200, loss: 0.02862568572163582\n",
      "epoch: 6, step: 300, loss: 0.028476843610405922\n",
      "epoch: 7, step: 0, loss: 0.03248909115791321\n",
      "epoch: 7, step: 100, loss: 0.017477212473750114\n",
      "epoch: 7, step: 200, loss: 0.01701279729604721\n",
      "epoch: 7, step: 300, loss: 0.021441766992211342\n",
      "epoch: 8, step: 0, loss: 0.018428798764944077\n",
      "epoch: 8, step: 100, loss: 0.01656557247042656\n",
      "epoch: 8, step: 200, loss: 0.03333979845046997\n",
      "epoch: 8, step: 300, loss: 0.017269305884838104\n",
      "epoch: 9, step: 0, loss: 0.006949608214199543\n",
      "epoch: 9, step: 100, loss: 0.013025907799601555\n",
      "epoch: 9, step: 200, loss: 0.011441216804087162\n",
      "epoch: 9, step: 300, loss: 0.017604494467377663\n",
      "Время обучения модели GRU: 0:03:04.080257\n",
      "Итоговый loss GRU на обучающей выборке: 0.01716022752225399\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size, \n",
    "                            shuffle=True, \n",
    "                            collate_fn=collate_fn,\n",
    "                            drop_last = True,\n",
    "                            )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = BidirRNN_model(batch['data'].to(device))\n",
    "        loss = loss_func(predict.view(-1, n_classes),           # loss function ожидает число предиктов и число классов\n",
    "                         batch['target'].to(device).view(-1),   # батч таргета вытягивается в 1 длинный тензор\n",
    "                         )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "BidirRNN_train_time = datetime.datetime.now() - start\n",
    "BidirRNN_train_loss = loss.item()\n",
    "print(f\"Время обучения модели RNN: {BidirRNN_train_time}\")\n",
    "print(f\"Итоговый loss RNN на обучающей выборке: {BidirRNN_train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', 'PUNCT', 'PROPN', 'PUNCT', 'ADV', 'ADJ', 'PRON', 'VERB', 'PRON']\n",
      "Время инференса модели RNN: 0:00:00.000998\n"
     ]
    }
   ],
   "source": [
    "phrase = 'So , Marius . At last I find you'\n",
    "words = phrase.split(' ')\n",
    "tokens = [dataset.word_vocab[w] for w in words]\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    BidirRNN_model.eval()\n",
    "    predict = BidirRNN_model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
    "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    BidirRNN_inference_time = datetime.datetime.now() - start\n",
    "\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l] for l in labels])\n",
    "print(f'Время инференса модели RNN: {BidirRNN_inference_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель Bidirectional GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirGRUPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        # делается эмбеддинг последовательности и целиком передается в RNN\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.clf = nn.Linear(hidden_dim * 2, n_classes)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.word_emb(x) # B x T x Emb_dim\n",
    "        # последовательность целиком передается в RNN\n",
    "        hidden, _ = self.rnn(emb)   # B x T x Hid, B x 1 x Hid\n",
    "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация модели, задание оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BidirGRU_model = BidirGRUPredictor(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
    "BidirGRU_model.train()\n",
    "optim = torch.optim.Adam(BidirGRU_model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трейн луп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 3.2253525257110596\n",
      "epoch: 0, step: 100, loss: 0.16711539030075073\n",
      "epoch: 0, step: 200, loss: 0.15875305235385895\n",
      "epoch: 0, step: 300, loss: 0.19771325588226318\n",
      "epoch: 1, step: 0, loss: 0.11242194473743439\n",
      "epoch: 1, step: 100, loss: 0.11645243316888809\n",
      "epoch: 1, step: 200, loss: 0.09003214538097382\n",
      "epoch: 1, step: 300, loss: 0.11728061735630035\n",
      "epoch: 2, step: 0, loss: 0.09017952531576157\n",
      "epoch: 2, step: 100, loss: 0.07458610832691193\n",
      "epoch: 2, step: 200, loss: 0.08346608281135559\n",
      "epoch: 2, step: 300, loss: 0.08584064990282059\n",
      "epoch: 3, step: 0, loss: 0.046209435909986496\n",
      "epoch: 3, step: 100, loss: 0.0766185075044632\n",
      "epoch: 3, step: 200, loss: 0.040786970406770706\n",
      "epoch: 3, step: 300, loss: 0.043938398361206055\n",
      "epoch: 4, step: 0, loss: 0.02935095503926277\n",
      "epoch: 4, step: 100, loss: 0.017926497384905815\n",
      "epoch: 4, step: 200, loss: 0.03432294726371765\n",
      "epoch: 4, step: 300, loss: 0.03164127841591835\n",
      "epoch: 5, step: 0, loss: 0.02312261052429676\n",
      "epoch: 5, step: 100, loss: 0.013990598730742931\n",
      "epoch: 5, step: 200, loss: 0.02582569606602192\n",
      "epoch: 5, step: 300, loss: 0.027037501335144043\n",
      "epoch: 6, step: 0, loss: 0.011555382050573826\n",
      "epoch: 6, step: 100, loss: 0.010791316628456116\n",
      "epoch: 6, step: 200, loss: 0.01872774213552475\n",
      "epoch: 6, step: 300, loss: 0.02195216529071331\n",
      "epoch: 7, step: 0, loss: 0.011858934536576271\n",
      "epoch: 7, step: 100, loss: 0.012658064253628254\n",
      "epoch: 7, step: 200, loss: 0.011309788562357426\n",
      "epoch: 7, step: 300, loss: 0.010448756627738476\n",
      "epoch: 8, step: 0, loss: 0.005164157599210739\n",
      "epoch: 8, step: 100, loss: 0.002377008320763707\n",
      "epoch: 8, step: 200, loss: 0.003535986877977848\n",
      "epoch: 8, step: 300, loss: 0.0033523901365697384\n",
      "epoch: 9, step: 0, loss: 0.003087087767198682\n",
      "epoch: 9, step: 100, loss: 0.0031073938589543104\n",
      "epoch: 9, step: 200, loss: 0.0029124559368938208\n",
      "epoch: 9, step: 300, loss: 0.005322860553860664\n",
      "Время обучения модели GRU: 0:04:11.358445\n",
      "Итоговый loss GRU на обучающей выборке: 0.0034210633020848036\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size, \n",
    "                            shuffle=True, \n",
    "                            collate_fn=collate_fn,\n",
    "                            drop_last = True,\n",
    "                            )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = BidirGRU_model(batch['data'].to(device))\n",
    "        loss = loss_func(predict.view(-1, n_classes),           # loss function ожидает число предиктов и число классов\n",
    "                         batch['target'].to(device).view(-1),   # батч таргета вытягивается в 1 длинный тензор\n",
    "                         )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "BidirGRU_train_time = datetime.datetime.now() - start\n",
    "BidirGRU_train_loss = loss.item()\n",
    "print(f\"Время обучения модели GRU: {BidirGRU_train_time}\")\n",
    "print(f\"Итоговый loss GRU на обучающей выборке: {BidirGRU_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', 'PUNCT', 'PROPN', 'PUNCT', 'ADV', 'ADJ', 'PRON', 'VERB', 'PRON']\n",
      "Время инференса модели RNN: 0:00:00.001995\n"
     ]
    }
   ],
   "source": [
    "phrase = 'So , Marius . At last I find you'\n",
    "words = phrase.split(' ')\n",
    "tokens = [dataset.word_vocab[w] for w in words]\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    BidirGRU_model.eval()\n",
    "    predict = BidirGRU_model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
    "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    BidirGRU_inference_time = datetime.datetime.now() - start\n",
    "\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l] for l in labels])\n",
    "print(f'Время инференса модели RNN: {BidirGRU_inference_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сведем результаты в таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Loss на обучающей выборке</th>\n",
       "      <th>Время инференса</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTM</th>\n",
       "      <td>0 days 00:03:09.865320</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>0 days 00:00:00.004986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bidirectional LSTM</th>\n",
       "      <td>0 days 00:05:12.256995</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0 days 00:00:00.005983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RNN</th>\n",
       "      <td>0 days 00:02:41.726528</td>\n",
       "      <td>0.041238</td>\n",
       "      <td>0 days 00:00:00.003990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bidirectional RNN</th>\n",
       "      <td>0 days 00:03:04.080257</td>\n",
       "      <td>0.017160</td>\n",
       "      <td>0 days 00:00:00.000998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRU</th>\n",
       "      <td>0 days 00:03:10.953375</td>\n",
       "      <td>0.027206</td>\n",
       "      <td>0 days 00:00:00.001995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bidirectional GRU</th>\n",
       "      <td>0 days 00:04:11.358445</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0 days 00:00:00.001995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Время обучения  Loss на обучающей выборке  \\\n",
       "LSTM               0 days 00:03:09.865320                   0.031769   \n",
       "Bidirectional LSTM 0 days 00:05:12.256995                   0.005682   \n",
       "RNN                0 days 00:02:41.726528                   0.041238   \n",
       "Bidirectional RNN  0 days 00:03:04.080257                   0.017160   \n",
       "GRU                0 days 00:03:10.953375                   0.027206   \n",
       "Bidirectional GRU  0 days 00:04:11.358445                   0.003421   \n",
       "\n",
       "                          Время инференса  \n",
       "LSTM               0 days 00:00:00.004986  \n",
       "Bidirectional LSTM 0 days 00:00:00.005983  \n",
       "RNN                0 days 00:00:00.003990  \n",
       "Bidirectional RNN  0 days 00:00:00.000998  \n",
       "GRU                0 days 00:00:00.001995  \n",
       "Bidirectional GRU  0 days 00:00:00.001995  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(data=[[LSTM_train_time, LSTM_train_loss, LSTM_inference_time],\n",
    "                            [BidirLSTM_train_time, BidirLSTM_train_loss, BidirLSTM_inference_time],\n",
    "                            [RNN_train_time, RNN_train_loss, RNN_inference_time],\n",
    "                            [BidirRNN_train_time, BidirRNN_train_loss, BidirRNN_inference_time],\n",
    "                            [GRU_train_time, GRU_train_loss, GRU_inference_time],\n",
    "                            [BidirGRU_train_time, BidirGRU_train_loss, BidirGRU_inference_time]],\n",
    "                      index = ['LSTM','Bidirectional LSTM','RNN','Bidirectional RNN','GRU','Bidirectional GRU'],\n",
    "                      columns = ['Время обучения', 'Loss на обучающей выборке', 'Время инференса'])\n",
    "result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "emb_dim = 256\n",
    "hidden = 256\n",
    "n_epochs = 10\n",
    "batch_size = 64  \n",
    "        Время обучения\t        Loss на обучающей выборке\tВремя инференса\n",
    "LSTM\t0 days 00:03:28.561591\t0.035101\t                0 days 00:00:00.001994\n",
    "RNN\t0 days 00:02:38.664945\t0.027867\t                0 days 00:00:00.003990\n",
    "GRU\t0 days 00:03:12.231247\t0.009496\t                0 days 00:00:00.002990\n",
    "\n",
    "emb_dim = 256\n",
    "hidden = 256\n",
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "        Время обучения\t        Loss на обучающей выборке\tВремя инференса\n",
    "LSTM\t0 days 00:02:26.290662\t0.026478\t                0 days 00:00:00.001994\n",
    "RNN\t0 days 00:01:44.315202\t0.045033\t                0 days 00:00:00.001995\n",
    "GRU\t0 days 00:02:17.878500\t0.037534\t                0 days 00:00:00.001995\n",
    "\n",
    "\n",
    "emb_dim = 256\n",
    "hidden = 256\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "\tВремя обучения\t        Loss на обучающей выборке\tВремя инференса\n",
    "LSTM\t0 days 00:04:04.160447\t0.017666\t                0 days 00:00:00.001994\n",
    "RNN\t0 days 00:03:48.225566\t0.019504\t                0 days 00:00:00.001995\n",
    "GRU\t0 days 00:04:35.424888\t0.009340\t                0 days 00:00:00.000997\n",
    "\n",
    "emb_dim = 256\n",
    "hidden = 128\n",
    "n_epochs = 10\n",
    "batch_size = 32   \n",
    "\n",
    "\tВремя обучения\t        Loss на обучающей выборке\tВремя инференса\n",
    "LSTM\t0 days 00:04:12.847231\t0.038909\t                0 days 00:00:00.001068\n",
    "RNN\t0 days 00:03:34.871725\t0.025090\t                0 days 00:00:00.001003\n",
    "GRU\t0 days 00:03:46.469728\t0.037704\t                0 days 00:00:00.000997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- уменьшение размера батча уменьшило loss (для обучаеющей выборки), но увеличило время обучения;\n",
    "- bidirectional удваивает время обучения (что логично), но loss на обучающей выборке уменьшается колоссально"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
